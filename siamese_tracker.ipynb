{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One GPU strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "# get available GPU\n",
    "devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(devices[0], True)\n",
    "gpu_name = \"GPU:0\"\n",
    "print(devices[0])\n",
    "\n",
    "# Only one gpu available to set to OneDeviceStrategy \n",
    "# Can be changed to MirroredStrategy if multiple GPU available\n",
    "strategy = tf.distribute.OneDeviceStrategy(device=gpu_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get VOC 2012 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get voc 2012 dataset\n",
    "splits = ['train[:80%]', 'train[80%:90%]', 'train[90%:]']\n",
    "\n",
    "(train_examples, validation_examples, test_examples), info = tfds.load('voc/2012', with_info=True, split=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeaturesDict({\n",
       "    'image': Image(shape=(None, None, 3), dtype=tf.uint8),\n",
       "    'image/filename': Text(shape=(), dtype=tf.string),\n",
       "    'labels': Sequence(ClassLabel(shape=(), dtype=tf.int64, num_classes=20)),\n",
       "    'labels_no_difficult': Sequence(ClassLabel(shape=(), dtype=tf.int64, num_classes=20)),\n",
       "    'objects': Sequence({\n",
       "        'bbox': BBoxFeature(shape=(4,), dtype=tf.float32),\n",
       "        'is_difficult': tf.bool,\n",
       "        'is_truncated': tf.bool,\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=20),\n",
       "        'pose': ClassLabel(shape=(), dtype=tf.int64, num_classes=5),\n",
       "    }),\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train examples: 5717, number of labels: 20\n"
     ]
    }
   ],
   "source": [
    "num_examples = info.splits['train'].num_examples\n",
    "num_classes = info.features['labels'].num_classes\n",
    "print(f\"Number of train examples: {num_examples}, number of labels: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess VOC2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize and normalize images\n",
    "@tf.function\n",
    "def format_image(tensor):\n",
    "    images = tf.image.resize(tensor['image'], IMAGE_SIZE) / 255.0\n",
    "    return images, tensor['objects']['label'], tensor['objects']['bbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = num_examples\n",
    "EPOCHS = 10\n",
    "IMAGE_SIZE = (224, 224)\n",
    "\n",
    "BATCH_SIZE_PER_REPLICA = 32\n",
    "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare batches\n",
    "BATCH_SIZE = 32\n",
    "train_batches = train_examples.shuffle(num_examples // 4).map(format_image).batch(GLOBAL_BATCH_SIZE).prefetch(1)\n",
    "validation_batches = validation_examples.map(format_image).batch(GLOBAL_BATCH_SIZE)\n",
    "test_batches = test_examples.map(format_image).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image of shapes: (224, 224, 3)\n",
      "Boxes with labels: [12 12 14 14]\n",
      "and localizations:\n",
      " [[0.26426426 0.168      0.9429429  0.688     ]\n",
      " [0.27327326 0.002      0.96997    0.584     ]\n",
      " [0.18618618 0.268      0.4234234  0.426     ]\n",
      " [0.23423423 0.354      0.5495495  0.526     ]]\n"
     ]
    }
   ],
   "source": [
    "test_data = list(train_examples)[2]\n",
    "test_result = format_image(test_data)\n",
    "\n",
    "print(f\"Image of shapes: {test_result[0].shape}\")\n",
    "print(f\"Boxes with labels: {test_result[1]}\")\n",
    "print(f\"and localizations:\\n {test_result[2]}\")\n",
    "\n",
    "del test_data, test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_datasets(strategy, train_batches, validation_batches, test_batches):\n",
    "    train_dist_dataset = strategy.experimental_distribute_dataset(train_batches)\n",
    "    val_dist_dataset = strategy.experimental_distribute_dataset(validation_batches)\n",
    "    test_dist_dataset = strategy.experimental_distribute_dataset(test_batches)\n",
    "\n",
    "    return train_dist_dataset, val_dist_dataset, test_dist_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.distribute.input_lib.DistributedDataset'>\n"
     ]
    }
   ],
   "source": [
    "train_dist_dataset, val_dist_dataset, test_dist_dataset = distribute_datasets(strategy, train_batches, validation_batches, test_batches)\n",
    "print(type(train_dist_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: (1, 224, 224, 3)\n",
      "Labels shape: (1, 2) and boxes shape: (1, 2, 4)\n"
     ]
    }
   ],
   "source": [
    "for batch, labels, boxes in test_dist_dataset:\n",
    "    break\n",
    "print(f\"Batch shape: {batch.shape}\")\n",
    "print(f\"Labels shape: {labels.shape} and boxes shape: {boxes.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, classes):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.tmp_layer = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.tmp_layer(inputs)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        reduction=tf.keras.losses.Reduction.NONE)\n",
    "    # or loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n",
    "    def compute_loss(labels, predictions):\n",
    "        per_example_loss = loss_object(labels, predictions)\n",
    "        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n",
    "\n",
    "    test_loss = tf.keras.metrics.Mean(name='test_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "        name='train_accuracy')\n",
    "    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "        name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model and optimizer must be created under `strategy.scope`.\n",
    "with strategy.scope():\n",
    "    model = MyModel(classes=num_classes)\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
